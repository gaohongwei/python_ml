from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel, PeftConfig
from datasets import load_dataset
import torch

# PEFTï¼ˆParameter-Efficient Fine-Tuningï¼‰,å‚æ•°é«˜æ•ˆå¾®è°ƒ
# æ”¯æŒå¦‚ LoRAã€QLoRA ç­‰æ–¹æ³•ï¼Œå¤§å¹…é™ä½è®­ç»ƒèµ„æºéœ€æ±‚ã€‚
# QLoRAï¼ˆQuantized LoRAï¼‰æ˜¯ LoRA çš„ä¸€ç§å˜ä½“ï¼Œç»“åˆäº†é‡åŒ–æŠ€æœ¯ä»¥è¿›ä¸€æ­¥å‡å°‘å†…å­˜å ç”¨ã€‚

# === Step 1: è½½å…¥åƒé—®ï¼ˆQwenï¼‰7Bæ¨¡å‹åŠå…¶åˆ†è¯å™¨ ===
base_model_name = "Qwen/Qwen1.5-0.5B"
tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)  # åŠ è½½åˆ†è¯å™¨

# åŠ è½½ Qwen æ¨¡å‹ï¼Œå¹¶å¯ç”¨ 4-bit é‡åŒ–ä»¥å‡å°‘å†…å­˜æ¶ˆè€—
model = AutoModelForCausalLM.from_pretrained(
    base_model_name,
    trust_remote_code=True,  # å¯ç”¨ä»è¿œç¨‹åŠ è½½ä»£ç ï¼Œæ”¯æŒQwençš„è‡ªå®šä¹‰ä»£ç 
    device_map="cpu"  # auto è‡ªåŠ¨é€‰æ‹©è®¾å¤‡ï¼ˆæ¯”å¦‚GPUï¼‰
    # å¯ç”¨4-bité‡åŒ–ï¼ˆæ˜¾è‘—å‡å°‘å†…å­˜å ç”¨ï¼‰
    # load_in_4bit=True,       
)

# === Step 2: å‡†å¤‡ QLoRA è®­ç»ƒï¼ˆå†»ç»“ä¸»æ¨¡å‹ï¼Œä»…è®­ç»ƒ LoRA Adapterï¼‰ ===
# QLoRA è®­ç»ƒæ­¥éª¤ï¼šæˆ‘ä»¬å†»ç»“åŸºç¡€æ¨¡å‹çš„æƒé‡ï¼Œåªå¯¹ LoRA adapter å±‚è¿›è¡Œè®­ç»ƒ
model = prepare_model_for_kbit_training(model)

# é…ç½® LoRA ç›¸å…³çš„å‚æ•°ï¼Œè¿™äº›å‚æ•°ä¼šå½±å“ adapter çš„ç»“æ„ä¸è®­ç»ƒæ•ˆæœ
lora_config = LoraConfig(
    r=8,                      # LoRAçŸ©é˜µçš„ç§©ï¼ˆä½ç§©è¿‘ä¼¼ï¼‰
    lora_alpha=32,            # LoRA ç¼©æ”¾å› å­ï¼ˆæ§åˆ¶ LoRA æ’ä»¶çš„æƒé‡å¤§å°ï¼‰
    target_modules=["c_attn", "q_proj", "v_proj"],  # é€‰æ‹©å“ªäº›æ¨¡å—æ’å…¥ LoRA adapterï¼ˆQwenä¸­çš„æŸäº›å±‚ï¼‰
    lora_dropout=0.05,        # LoRA adapterçš„dropoutç‡
    bias="none",              # ä¸ä¸º adapter å±‚çš„åç½®é¡¹æ·»åŠ å‚æ•°
    task_type="CAUSAL_LM"     # è®¾å®šä»»åŠ¡ç±»å‹ä¸ºå› æœè¯­è¨€å»ºæ¨¡ï¼ˆCausal Language Modelingï¼‰
)

# å°† LoRA é…ç½®åº”ç”¨åˆ°æ¨¡å‹ï¼Œå¾—åˆ°ä¸€ä¸ªå¸¦æœ‰ LoRA adapter çš„æ¨¡å‹
model = get_peft_model(model, lora_config)

# === Step 3: åŠ è½½åŒ»å­¦é—®ç­”æ•°æ®é›† pubmed_qaï¼ˆæ¥è‡ª Hugging Faceï¼‰ ===
# è¿™é‡Œé€‰æ‹©äº† pubmed_qa æ•°æ®é›†ï¼Œå®ƒåŒ…å«åŒ»å­¦é¢†åŸŸçš„é—®é¢˜å’Œç­”æ¡ˆï¼Œé€‚åˆç”¨äºé—®ç­”ä»»åŠ¡
# è¿™é‡Œåªå–å‰1000æ¡æ•°æ®ç”¨äºæµ‹è¯•
dataset = load_dataset("pubmed_qa", "pqa_labeled", split="train[:1000]")  


# æ•°æ®é¢„å¤„ç†ï¼šæˆ‘ä»¬å°†é—®é¢˜å’ŒèƒŒæ™¯æ‹¼æ¥æˆä¸€ä¸ªè¾“å…¥æ–‡æœ¬ï¼Œä½œä¸º QLoRA æ¨¡å‹çš„è¾“å…¥
def preprocess(example):
    # æ‹¼æ¥é—®é¢˜å’ŒèƒŒæ™¯ä¿¡æ¯ï¼Œå½¢æˆæ¨¡å‹è¾“å…¥æ ¼å¼
    input_text = f"é—®é¢˜ï¼š{example['question']}\nèƒŒæ™¯ï¼š{example['context']}\nè¯·å›ç­”ï¼š"
    return tokenizer(input_text, padding="max_length", truncation=True, max_length=512)

# å¯¹æ•°æ®é›†è¿›è¡Œæ‰¹é‡å¤„ç†
tokenized_dataset = dataset.map(preprocess)

# === Step 4: è®¾ç½®è®­ç»ƒå‚æ•° ===
# è¿™é‡Œä½¿ç”¨çš„æ˜¯ Hugging Face çš„ Trainer æ¥è¿›è¡Œè®­ç»ƒ
training_args = TrainingArguments(
    per_device_train_batch_size=2,     # æ¯ä¸ªè®¾å¤‡ä¸Šçš„è®­ç»ƒæ‰¹æ¬¡å¤§å°ï¼ˆæ ¹æ®æ˜¾å­˜å¤§å°å¯ä»¥è°ƒèŠ‚ï¼‰
    gradient_accumulation_steps=4,     # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆç”¨äºå¤§æ‰¹æ¬¡è®­ç»ƒï¼‰
    warmup_steps=5,                   # å­¦ä¹ ç‡é¢„çƒ­æ­¥éª¤æ•°
    max_steps=30,                     # è®­ç»ƒæ­¥æ•°ï¼ˆä¸ºäº†æ¼”ç¤ºï¼Œè¿™é‡Œè®¾ç½®ä¸º30æ­¥ï¼Œå®é™…æƒ…å†µå¯ä»¥æ›´é•¿ï¼‰
    learning_rate=2e-4,               # å­¦ä¹ ç‡
    fp16=True,                         # å¯ç”¨åŠç²¾åº¦è®­ç»ƒï¼Œå‡å°‘æ˜¾å­˜æ¶ˆè€—
    logging_steps=5,                   # æ—¥å¿—è®°å½•é¢‘ç‡
    output_dir="./qwen_qlora_medical", # ä¿å­˜è®­ç»ƒç»“æœçš„ç›®å½•
    report_to="none"                   # ä¸ä¸Šä¼ è®­ç»ƒæ—¥å¿—åˆ°ä»»ä½•å¹³å°
)

# === Step 5: å¯åŠ¨ Trainer è¿›è¡Œ QLoRA å¾®è°ƒ ===
# ä½¿ç”¨ Trainer ç±»æ¥è¿›è¡Œæ¨¡å‹çš„å¾®è°ƒè®­ç»ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,  # è®­ç»ƒæ•°æ®é›†
    tokenizer=tokenizer                # åˆ†è¯å™¨
)

# å¼€å§‹è®­ç»ƒ
trainer.train()

# === Step 6: ä¿å­˜è®­ç»ƒå¾—åˆ°çš„ LoRA Adapter å’Œ Tokenizer ===
# è®­ç»ƒå®Œæˆåï¼Œä¿å­˜ LoRA adapter æƒé‡å’Œåˆ†è¯å™¨
model.save_pretrained("./qwen_med_qlora_adapter")  # ä¿å­˜æ¨¡å‹
tokenizer.save_pretrained("./qwen_med_qlora_adapter")  # ä¿å­˜åˆ†è¯å™¨
print("âœ… LoRA adapter å·²ä¿å­˜åˆ° ./qwen_med_qlora_adapter")

# === Step 7: åŠ è½½ä¿å­˜çš„ Adapter è¿›è¡Œæ¨ç† ===
# é‡æ–°åŠ è½½ä¹‹å‰ä¿å­˜çš„ LoRA adapter æƒé‡ï¼Œå¹¶è¿›è¡Œæ¨ç†
adapter_path = "./qwen_med_qlora_adapter"
config = PeftConfig.from_pretrained(adapter_path)

# åŠ è½½åŸºç¡€æ¨¡å‹
base_model = AutoModelForCausalLM.from_pretrained(
    config.base_model_name_or_path,
    trust_remote_code=True,  # å¯ç”¨è‡ªå®šä¹‰ä»£ç æ”¯æŒ
    load_in_4bit=True,       # 4-bité‡åŒ–
    device_map="auto"        # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
)

# è½½å…¥ LoRA adapter åˆ°æ¨¡å‹
model = PeftModel.from_pretrained(base_model, adapter_path)

# ç¤ºä¾‹æ¨ç†ï¼šæå‡ºä¸€ä¸ªåŒ»å­¦ç›¸å…³çš„é—®é¢˜ï¼Œæ¨¡å‹ä¼šç”Ÿæˆç­”æ¡ˆ
input_text = "é—®é¢˜ï¼šä»€ä¹ˆæ˜¯é«˜è¡€å‹ï¼Ÿ\nèƒŒæ™¯ï¼šé«˜è¡€å‹æ˜¯ä¸€ç§å¸¸è§çš„æ…¢æ€§ç—…ï¼Œæ‚£è€…è¡€å‹æŒç»­å‡é«˜ã€‚\nè¯·å›ç­”ï¼š"
inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=100)  # ç”Ÿæˆæœ€å¤š100ä¸ªæ–°token
print("ğŸ§  æ¨¡å‹è¾“å‡ºï¼š")
print(tokenizer.decode(outputs[0], skip_special_tokens=True))  # è§£ç å¹¶è¾“å‡ºç»“æœ

# === Step 8: åˆå¹¶ adapter ç”Ÿæˆéƒ¨ç½²ç‰ˆæ¨¡å‹ï¼ˆå¯è½¬ä¸º ONNXã€GGUF ç­‰æ ¼å¼ï¼‰ ===
# å¦‚æœä½ å‡†å¤‡éƒ¨ç½²æ¨¡å‹ï¼Œå¯ä»¥å°† LoRA adapter å’ŒåŸºç¡€æ¨¡å‹åˆå¹¶ä¸ºä¸€ä¸ªå®Œæ•´çš„æ¨¡å‹
merged_model = model.merge_and_unload()  # åˆå¹¶ LoRA adapter åˆ°åŸºç¡€æ¨¡å‹
merged_model.save_pretrained("./qwen_med_merged_model")  # ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
tokenizer.save_pretrained("./qwen_med_merged_model")    # ä¿å­˜åˆ†è¯å™¨
print("ğŸ“¦ éƒ¨ç½²ç‰ˆæ¨¡å‹å·²ä¿å­˜åˆ° ./qwen_med_merged_model")
