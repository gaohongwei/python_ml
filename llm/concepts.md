# LLM concepts

## transformers,Hugging Face

- Python åº“ï¼ˆtoolkit / frameworkï¼‰
- AutoModel, AutoTokenizer ç»Ÿä¸€åŠ è½½æ¨¡å‹å’Œ tokenizer
- Trainer æ¨¡å‹è®­ç»ƒæ¡†æ¶ï¼Œç±»ä¼¼ sklearn çš„ .fit()
- datasets é…å¥—çš„è®­ç»ƒ/æµ‹è¯•æ•°æ®åŠ è½½å·¥å…·ï¼ˆéœ€ datasets åº“ï¼‰
- pipeline å¼€ç®±å³ç”¨çš„æ¨ç†å°è£…ï¼Œé€‚åˆå¿«é€Ÿæµ‹è¯•ä»»åŠ¡

## sample code

```
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# æŒ‡å®šé¢„è®­ç»ƒæ¨¡å‹åç§°ï¼ˆå¯æ›¿æ¢ä¸ºå…¶ä»–æ”¯æŒçš„æ¨¡å‹ï¼‰
###  å°å‹ä¸­æ–‡æ¨¡å‹
model_name = "Qwen/Qwen1.5-0.5B"
model_name =  "microsoft/Phi-3-mini-4k-instruct"
#ä»æœ¬åœ°åŠ è½½
model_name = "./models/qwen-0.5b"

# åŠ è½½å› æœè¯­è¨€æ¨¡å‹ï¼ˆç”¨äºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼‰
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="cuda",           # è‡ªåŠ¨å°†æ¨¡å‹åŠ è½½åˆ° GPU
    torch_dtype="auto",          # è‡ªåŠ¨é€‰æ‹©æœ€åˆé€‚çš„æ•°æ®ç±»å‹ï¼ˆå¦‚ float16ï¼‰
    trust_remote_code=True       # å…è®¸åŠ è½½è¿œç¨‹æ¨¡å‹ä¸­è‡ªå®šä¹‰çš„ä»£ç 
)

# åŠ è½½å¯¹åº”çš„åˆ†è¯å™¨ï¼ˆTokenizerï¼‰
tokenizer = AutoTokenizer.from_pretrained(model_name)

# æ„å»ºä¸€ä¸ªæ–‡æœ¬ç”Ÿæˆçš„ pipeline
pipe = pipeline(
    task="text-generation",      # æŒ‡å®šä»»åŠ¡ç±»å‹ä¸ºâ€œæ–‡æœ¬ç”Ÿæˆâ€
    model=model,                 # ä½¿ç”¨ä¸Šé¢åŠ è½½çš„æ¨¡å‹
    tokenizer=tokenizer,         # ä½¿ç”¨ç›¸åº”çš„ tokenizer
    return_full_text=True,       # è¿”å›å®Œæ•´æ–‡æœ¬ï¼ˆåŒ…æ‹¬è¾“å…¥æç¤ºï¼‰
    max_new_tokens=500,          # æœ€å¤šç”Ÿæˆ 500 ä¸ªæ–° token
    do_sample=True               # å¯ç”¨é‡‡æ ·ï¼ˆç”Ÿæˆå†…å®¹æ›´éšæœºï¼‰
)

# ç¤ºä¾‹è°ƒç”¨ï¼ˆå¯é€‰ï¼‰
# output = pipe("è¯·å†™ä¸€ä¸ªå…³äºäººå·¥æ™ºèƒ½çš„æ•…äº‹")[0]['generated_text']
# print(output)

```

## Chapter 2. Tokens and Embeddings

### workflow

text -> token_ids->LLM->token_ids->text

### tokens

    æ¯ä¸ª token ID æ˜¯ä¸€ä¸ªå”¯ä¸€çš„ç¼–å·ï¼Œå¯¹åº”æŸä¸€ä¸ªtokenã€‚
    è¿™ä¸ª token å¯ä»¥æ˜¯ï¼šä¸€ä¸ªå­—ç¬¦ã€ä¸€ä¸ªå•è¯ï¼Œæˆ–è€…ä¸€ä¸ªè¯çš„ä¸€éƒ¨åˆ†ï¼ˆæ¯”å¦‚è¯æ ¹ã€å‰ç¼€ã€åç¼€ï¼‰ã€‚
    è¿™äº› ID æŒ‡å‘ tokenizer å†…éƒ¨çš„ä¸€å¼ è¯è¡¨ï¼ˆvocabularyï¼‰ï¼Œè¿™å¼ è¯è¡¨è®°å½•äº†æ‰€æœ‰å®ƒâ€œè®¤è¯†â€çš„ tokenã€‚

### å†³å®š Tokenizer åˆ†è¯æ–¹å¼çš„ä¸‰å¤§å…³é”®å› ç´ 

- é€‰æ‹©åˆ†è¯ç®—æ³•ï¼ˆæ¨¡å‹è®¾è®¡é˜¶æ®µï¼‰
  - BPEï¼ˆByte Pair Encodingï¼Œå­—èŠ‚å¯¹ç¼–ç ï¼‰
  - WordPiece
  - BERT, åŸºäº Transformer ç¼–ç å™¨æ¶æ„ çš„è¯­è¨€ç†è§£æ¨¡å‹,Bidirectional,åŒæ—¶å‘å·¦å³çœ‹
- åˆ†è¯å™¨è®¾è®¡ç»†èŠ‚
- è®­ç»ƒè¯­æ–™çš„å½±å“

### attention_mask

### attention_mask æ˜¯ç”¨äºæŒ‡ç¤ºæ¨¡å‹åœ¨è®¡ç®—è‡ªæ³¨æ„åŠ›æ—¶ï¼Œå“ªäº›ä½ç½®éœ€è¦è¢«å…³æ³¨ï¼ˆå³å“ªäº› token æ˜¯æœ‰æ•ˆçš„ï¼Œå“ªäº›åº”è¯¥è¢«å¿½ç•¥ï¼‰çš„ä¸€ç§æ ‡è®°ã€‚åœ¨å¾ˆå¤š NLP æ¨¡å‹ä¸­ï¼Œå°¤å…¶æ˜¯åœ¨ Transformer æ¶æ„ä¸­ï¼Œattention mask æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„æ¦‚å¿µã€‚

ğŸŒŸ è¯¦ç»†è§£é‡Šï¼š
attention_mask æ˜¯ä¸€ä¸ªä¸ input_ids é•¿åº¦ç›¸åŒçš„äºŒå€¼åºåˆ—ï¼ˆé€šå¸¸æ˜¯ä¸€ä¸ªå¼ é‡ï¼‰ï¼Œå…¶å€¼è¡¨ç¤ºå“ªäº› token æ˜¯æœ‰æ•ˆçš„ï¼Œå“ªäº› token æ˜¯å¡«å……ï¼ˆpaddingï¼‰çš„ä½ç½®ã€‚

1 è¡¨ç¤ºè¯¥ä½ç½®æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ tokenï¼Œæ¨¡å‹åº”è¯¥è€ƒè™‘è¯¥ tokenã€‚

0 è¡¨ç¤ºè¯¥ä½ç½®æ˜¯å¡«å……ä½ç½®ï¼Œæ¨¡å‹åº”è¯¥å¿½ç•¥è¯¥ä½ç½®ã€‚

ä¸ºä»€ä¹ˆéœ€è¦ attention_maskï¼Ÿ
åœ¨å¤„ç†å˜é•¿æ–‡æœ¬æ—¶ï¼Œé€šå¸¸æˆ‘ä»¬ä¼šå¯¹è¾ƒçŸ­çš„æ–‡æœ¬è¿›è¡Œ å¡«å……ï¼Œä½¿å…¶é•¿åº¦ä¸æœ€é•¿çš„æ–‡æœ¬ç›¸åŒã€‚å¡«å……çš„éƒ¨åˆ†å¹¶æ²¡æœ‰å®é™…çš„è¯­ä¹‰å†…å®¹ï¼Œä½†åœ¨è®¡ç®—è‡ªæ³¨æ„åŠ›ï¼ˆself-attentionï¼‰æ—¶ï¼Œæ¨¡å‹åº”è¯¥å¿½ç•¥è¿™äº›å¡«å……éƒ¨åˆ†çš„å½±å“ã€‚attention_mask å°±æ˜¯ç”¨æ¥æ ‡è®°è¿™äº›å¡«å……ä½ç½®çš„ã€‚

### Token Embeddings

### Text Embeddings

## Chapter 3. Looking Inside Large Language Models

### Sample code

from transformers import AutoModelForCausalLM, AutoTokenizer

### å°å‹ä¸­æ–‡æ¨¡å‹

```
model_name = "Qwen/Qwen1.5-0.5B"
model = AutoModelForCausalLM.from_pretrained(model_name,
    device_map="cpu",# auto
    torch_dtype="auto",
    trust_remote_code=True,
)

# Create a pipeline

generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    return_full_text= False ,
    max_new_tokens=50,
    do_sample= False,
)
```

The model does not generate the text all in one operation;
it actually generates one token at a time.
Each token generation step is one forward pass through the model
After each token generation, we tweak the input prompt for the next generation step by appending the output token to the end of the input prompt.
Software around the neural network basically runs it in a loop to sequentially expand the generated text until completion.

#### The Components of the Forward Pass

In addition to the loop, two key internal components are the tokenizer and the language modeling head (LM head).

## Chapter 12. Fine-Tuning Generation Models

LoRA is a technique that (like adapters) only requires updating a small set of parameters.
It creates a small subset of the base model to fine-tune instead of adding layers to the model.
LoRA requires only fine-tuning a small set of parameters that can be kept separately from the base LLM. And it update a small part of the base model.

We create this subset of parameters by approximating large matrices that accompany the original LLM with smaller matrices.

### Tuning with QLoRA

å°†åŸå§‹æ¨¡å‹å‹ç¼©ä¸º 4-bit çš„å…³é”®æµç¨‹ï¼ˆæ›´æ–°ç‰ˆï¼‰
ğŸ”‘ æ ¸å¿ƒåˆ—è¡¨è¦ç‚¹ï¼š
ç›®æ ‡ï¼šå°†åŸå§‹æ¨¡å‹ï¼ˆå¦‚ Qwenã€LLaMAï¼‰åŠ è½½ä¸ºå‹ç¼©çš„ 4-bit æ ¼å¼ï¼Œä»¥ä¾¿èŠ‚çœæ˜¾å­˜ã€åŠ é€Ÿè®­ç»ƒã€‚

å…³é”®å·¥å…·ï¼šä½¿ç”¨ bitsandbytes åº“ + transformers ä¸­çš„ BitsAndBytesConfigã€‚

å‹ç¼©å¯¹è±¡ï¼šå‹ç¼©çš„æ˜¯ã€ŒåŸå§‹æ¨¡å‹ã€çš„æƒé‡ï¼ŒLoRA Adapter ä¿æŒå…¨ç²¾åº¦ï¼ˆfloat32 / float16ï¼‰ã€‚

æ¨èé…ç½®ï¼ˆåŸºäº QLoRA è®ºæ–‡ï¼‰ï¼š

load_in_4bit=Trueï¼šå¯ç”¨ 4-bit åŠ è½½

bnb_4bit_quant_type="nf4"ï¼šä½¿ç”¨å½’ä¸€åŒ–æµ®ç‚¹ï¼ˆNormalized Float 4ï¼‰

bnb_4bit_use_double_quant=Trueï¼šåŒé‡é‡åŒ–ï¼Œæå‡å‹ç¼©ç‡

bnb_4bit_compute_dtype=torch.bfloat16ï¼šä½¿ç”¨ bfloat16 åŠ é€Ÿæ¨ç†

ä½œç”¨ï¼š

æ˜¾è‘—èŠ‚çœ GPU æ˜¾å­˜ï¼ˆ4 å€+ï¼‰

ä¿ç•™æ€§èƒ½ï¼Œä¾¿äºåœ¨æ™®é€šæ¶ˆè´¹çº§æ˜¾å¡ä¸Šåšå¾®è°ƒ

æ˜¯ QLoRA å¾—ä»¥ã€Œè½»é‡è®­ç»ƒå¤§æ¨¡å‹ã€çš„åŸºç¡€

## try

import torch pipeline
import torch pipeline
AutoModelForCausalLM, AutoTokenizer,

# Load model and tokenizer

model_name="abc"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model)

# Create a pipeline

pipe = pipeline(
"text-generation",
model=model,
tokenizer=tokenizer,
return_full_text= False,
max_new_tokens=500,
do_sample= False)

messages = [
{"role": "user", "content": "Create a funny joke about
chickens."}
]

# Generate the output

output = pipe(messages)
print(output[0]["generated_text"])

Under the hood, transformers.pipeline first converts our messages into a specific prompt template. We can explore this process by accessing the underlying tokenizer:
Why don't chickens like to go to the gym? Because they can't
crack the egg-sistence of it!
eslaF eslaF
eurT

# Apply prompt template

prompt = pipe.tokenizer.apply_chat_template(messages, tokenize= )
print(prompt)
