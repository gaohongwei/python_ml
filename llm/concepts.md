### transformers,Hugging Face

- Python åº“ï¼ˆtoolkit / frameworkï¼‰
- AutoModel, AutoTokenizer ç»Ÿä¸€åŠ è½½æ¨¡å‹å’Œ tokenizer çš„æ¥
- Trainer æ¨¡å‹è®­ç»ƒæ¡†æ¶ï¼Œç±»ä¼¼ sklearn çš„ .fit()
- datasets é…å¥—çš„è®­ç»ƒ/æµ‹è¯•æ•°æ®åŠ è½½å·¥å…·ï¼ˆéœ€ datasets åº“ï¼‰
- pipeline å¼€ç®±å³ç”¨çš„æ¨ç†å°è£…ï¼Œé€‚åˆå¿«é€Ÿæµ‹è¯•ä»»åŠ¡
- Common usage:

```
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# æŒ‡å®šé¢„è®­ç»ƒæ¨¡å‹åç§°ï¼ˆå¯æ›¿æ¢ä¸ºå…¶ä»–æ”¯æŒçš„æ¨¡å‹ï¼‰
###  å°å‹ä¸­æ–‡æ¨¡å‹
model_name = "Qwen/Qwen1.5-0.5B"
model_name =  "microsoft/Phi-3-mini-4k-instruct"
#ä»æœ¬åœ°åŠ è½½
model_name = "./models/qwen-0.5b"

# åŠ è½½å› æœè¯­è¨€æ¨¡å‹ï¼ˆç”¨äºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼‰
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="cuda",           # è‡ªåŠ¨å°†æ¨¡å‹åŠ è½½åˆ° GPU
    torch_dtype="auto",          # è‡ªåŠ¨é€‰æ‹©æœ€åˆé€‚çš„æ•°æ®ç±»å‹ï¼ˆå¦‚ float16ï¼‰
    trust_remote_code=True       # å…è®¸åŠ è½½è¿œç¨‹æ¨¡å‹ä¸­è‡ªå®šä¹‰çš„ä»£ç 
)

# åŠ è½½å¯¹åº”çš„åˆ†è¯å™¨ï¼ˆTokenizerï¼‰
tokenizer = AutoTokenizer.from_pretrained(model_name)

# æ„å»ºä¸€ä¸ªæ–‡æœ¬ç”Ÿæˆçš„ pipeline
pipe = pipeline(
    task="text-generation",      # æŒ‡å®šä»»åŠ¡ç±»å‹ä¸ºâ€œæ–‡æœ¬ç”Ÿæˆâ€
    model=model,                 # ä½¿ç”¨ä¸Šé¢åŠ è½½çš„æ¨¡å‹
    tokenizer=tokenizer,         # ä½¿ç”¨ç›¸åº”çš„ tokenizer
    return_full_text=True,       # è¿”å›å®Œæ•´æ–‡æœ¬ï¼ˆåŒ…æ‹¬è¾“å…¥æç¤ºï¼‰
    max_new_tokens=500,          # æœ€å¤šç”Ÿæˆ 500 ä¸ªæ–° token
    do_sample=True               # å¯ç”¨é‡‡æ ·ï¼ˆç”Ÿæˆå†…å®¹æ›´éšæœºï¼‰
)

# ç¤ºä¾‹è°ƒç”¨ï¼ˆå¯é€‰ï¼‰
# output = pipe("è¯·å†™ä¸€ä¸ªå…³äºäººå·¥æ™ºèƒ½çš„æ•…äº‹")[0]['generated_text']
# print(output)

```

### datasets,Hugging Face

- list_datasets
  > > > from datasets import list_datasets
  > > > all_datasets = list_datasets()
  > > > <stdin>:1: FutureWarning: list_datasets is deprecated and will be removed in the next major version of datasets. Use 'huggingface_hub.list_datasets' instead.

from transformer import
AutoModelForCausalLLM,
AutoTokenizer,
pipeline

model = AutoModelForCausalLLM.from_pretrained()
tokenizer = AutoTokenizer.from_pretrained()

tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")
prompt = "Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.<|assistant|>"

input_ids = tokenizer(prompt,return_tensors="pt").input_ids.to("cpu")

generation_output = model.generate(
input_ids=input_ids,
max_new_tokens=20
)

## concept

### attention_mask æ˜¯ç”¨äºæŒ‡ç¤ºæ¨¡å‹åœ¨è®¡ç®—è‡ªæ³¨æ„åŠ›æ—¶ï¼Œå“ªäº›ä½ç½®éœ€è¦è¢«å…³æ³¨ï¼ˆå³å“ªäº› token æ˜¯æœ‰æ•ˆçš„ï¼Œå“ªäº›åº”è¯¥è¢«å¿½ç•¥ï¼‰çš„ä¸€ç§æ ‡è®°ã€‚åœ¨å¾ˆå¤š NLP æ¨¡å‹ä¸­ï¼Œå°¤å…¶æ˜¯åœ¨ Transformer æ¶æ„ä¸­ï¼Œattention mask æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„æ¦‚å¿µã€‚

ğŸŒŸ è¯¦ç»†è§£é‡Šï¼š
attention_mask æ˜¯ä¸€ä¸ªä¸ input_ids é•¿åº¦ç›¸åŒçš„äºŒå€¼åºåˆ—ï¼ˆé€šå¸¸æ˜¯ä¸€ä¸ªå¼ é‡ï¼‰ï¼Œå…¶å€¼è¡¨ç¤ºå“ªäº› token æ˜¯æœ‰æ•ˆçš„ï¼Œå“ªäº› token æ˜¯å¡«å……ï¼ˆpaddingï¼‰çš„ä½ç½®ã€‚

1 è¡¨ç¤ºè¯¥ä½ç½®æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ tokenï¼Œæ¨¡å‹åº”è¯¥è€ƒè™‘è¯¥ tokenã€‚

0 è¡¨ç¤ºè¯¥ä½ç½®æ˜¯å¡«å……ä½ç½®ï¼Œæ¨¡å‹åº”è¯¥å¿½ç•¥è¯¥ä½ç½®ã€‚

ä¸ºä»€ä¹ˆéœ€è¦ attention_maskï¼Ÿ
åœ¨å¤„ç†å˜é•¿æ–‡æœ¬æ—¶ï¼Œé€šå¸¸æˆ‘ä»¬ä¼šå¯¹è¾ƒçŸ­çš„æ–‡æœ¬è¿›è¡Œ å¡«å……ï¼Œä½¿å…¶é•¿åº¦ä¸æœ€é•¿çš„æ–‡æœ¬ç›¸åŒã€‚å¡«å……çš„éƒ¨åˆ†å¹¶æ²¡æœ‰å®é™…çš„è¯­ä¹‰å†…å®¹ï¼Œä½†åœ¨è®¡ç®—è‡ªæ³¨æ„åŠ›ï¼ˆself-attentionï¼‰æ—¶ï¼Œæ¨¡å‹åº”è¯¥å¿½ç•¥è¿™äº›å¡«å……éƒ¨åˆ†çš„å½±å“ã€‚attention_mask å°±æ˜¯ç”¨æ¥æ ‡è®°è¿™äº›å¡«å……ä½ç½®çš„ã€‚
