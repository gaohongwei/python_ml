1. 超参数调优 (Hyperparameter Tuning)
   超参数调优是机器学习中非常重要的一个步骤。它通过不断调整模型的超参数（例如学习率、dropout 率等），来寻找最适合当前问题的模型配置。超参数调优的目标是提高模型的性能。

学习率 (Learning Rate)：决定了每次参数更新的步长。如果学习率过大，可能会错过最优解；如果学习率过小，训练过程可能会非常缓慢。
Dropout 率 (Dropout Rate)：一种正则化方法，用于防止过拟合。在训练过程中，随机“丢弃”一定比例的神经元（即将其激活值设为 0），以防止模型对训练数据的过拟合。 2. 按批次加载数据 (Batch Data Loading)
在深度学习中，为了节省内存和提高计算效率，通常不会一次性将所有数据加载到内存中，而是采用批次加载（Batch Loading）。这样做有以下好处：

内存效率：特别是当数据集较大时，将整个数据集加载到内存中是不现实的。按批次加载可以显著减少内存消耗。
提高计算效率：批量处理数据更符合现代硬件（例如 GPU）的计算架构。GPU 通常能一次处理多个数据样本，训练过程也能得到加速。
在代码中，我们通过调整批次大小和按批次加载数据来优化训练过程。

3. 模型训练 (Model Training)
   训练一个深度学习模型，意味着通过优化模型的参数（例如卷积层中的权重），让它尽可能地减少预测误差。在训练过程中，我们需要：

前向传播 (Forward Pass)：将输入数据通过神经网络进行处理，得到预测结果。
计算损失 (Loss Calculation)：通过某种损失函数（例如交叉熵损失）计算模型预测结果与真实标签之间的差距。
反向传播 (Backward Pass)：根据损失的反向传播来更新模型的参数（通过梯度下降法）。 4. 优化器 (Optimizer)
优化器决定了如何根据损失函数来更新模型参数。常见的优化器包括：

Adam (Adaptive Moment Estimation)：是一种常用的优化算法，结合了 Momentum 和 RMSProp 的优点，能自适应地调整每个参数的学习率，通常效果非常好。
在代码中，我们用 Adam 优化器来更新模型的权重。

5. 模型保存 (Model Saving)
   训练完成后，我们需要保存模型，以便在之后的实验或推理中使用。在 PyTorch 中，可以通过 torch.save(model.state_dict(), path) 来保存模型的参数（权重）。

6. 模型加载 (Model Loading)
   加载模型的过程通常包括：

加载模型的结构（通过代码定义模型架构）。
加载模型的权重（通过 model.load_state_dict() 将保存的权重加载到模型中）。
在此代码中，我们建议仅在 fine_tune 阶段加载外部模型（从存储路径加载模型），以便继续训练。

7. 批次大小 (Batch Size)
   批次大小是指每次迭代中传入模型的数据量。常见的批次大小有 32、64、128 等。在训练时，按批次处理数据，而不是一次处理所有数据。批次越大，每次迭代的计算量越大，但可能导致训练过程更稳定；批次越小，计算量小但可能训练更不稳定。

8. 增量学习 (Incremental Learning)
   增量学习是指在模型训练过程中，逐渐增加数据集的大小，而不是一次性训练。通过这种方式，模型能够在新的数据上进行微调，而不需要从头开始训练。增量学习在数据集非常大、无法一次性加载到内存的情况下非常有效。

9. Fine-tuning（微调）
   微调（Fine-tuning）是指在已训练的模型基础上，针对新任务或数据进行调整。在深度学习中，常常用预训练模型（例如在大数据集上预训练的模型）来进行微调，这样可以避免从头开始训练，并且通常能得到更好的效果。

10. 实验的重复性与超参数的依赖
    在多个实验中，确保实验的一致性非常重要。每次实验都应该有明确的超参数范围，并且每次更新的参数应基于上次实验的结果来进一步调整。

## questions

为什么选择 dropout_rate 和 learning_rate？
学习率 (Learning Rate)
学习率是最重要的超参数之一，它直接影响模型的收敛速度和稳定性。学习率太大可能会导致模型无法收敛，而学习率太小则可能使训练过程变得非常缓慢。因此，找到一个合适的学习率至关重要。
Dropout 率 (Dropout Rate)
Dropout 是一种常用的正则化技术，它通过随机“丢弃”神经网络中的一些神经元来防止过拟合。特别是在训练数据量较小或模型复杂度较高时，dropout 是避免过拟合的有效方法。调整 dropout 率可以帮助在避免过拟合的同时，保持模型的表达能力。
这两个超参数是训练过程中最常见的调整项，它们对模型的影响比较直接，因此在超参数调优时，通常首先考虑这两者。

其他常见的 CNN 超参数
除了 learning_rate 和 dropout_rate，CNN 模型中还涉及很多其他超参数，常见的有：

批次大小 (Batch Size)

批次大小决定了每次训练时，输入到模型中的样本数量。较小的批次大小可以让训练过程更加频繁更新，但可能会导致训练不稳定；较大的批次大小通常能提高计算效率，但可能导致模型收敛速度较慢。
卷积层的滤波器数量 (Number of Filters in Conv Layers)

在卷积神经网络中，卷积层的滤波器（或称卷积核）数量决定了模型的表达能力。增加滤波器数量可以增加模型的复杂度，但也可能带来过拟合的风险。
卷积核的大小 (Filter Size)

卷积核的大小决定了每次卷积操作时感受野的大小。常见的卷积核大小有 3x3、5x5 等。较大的卷积核能够捕捉更广泛的特征，但也会增加计算量。
池化层的类型和大小 (Pooling Type and Size)

池化层用于下采样，减少特征图的尺寸。常见的池化层类型有最大池化（Max Pooling）和平均池化（Average Pooling），而池化窗口的大小通常为 2x2 或 3x3。
权重初始化方法 (Weight Initialization)

神经网络的权重初始化方法对模型的收敛速度和性能有影响。常见的初始化方法包括 Xavier 初始化、He 初始化等。
优化器 (Optimizer)

除了 Adam，还有很多优化器可供选择，比如 SGD（随机梯度下降）、RMSProp、Adagrad 等。不同的优化器可能适用于不同的问题，需要根据具体情况选择。
为什么在 run_tuning 只调优这两个参数？
简化问题：为了让超参数调优更高效且易于实现，我们可以首先对最关键的参数（如学习率和 dropout 率）进行调优。这样可以在相对较短的时间内找到一个较好的模型配置，然后再根据需要进一步调优其他超参数。

逐步调优：通常情况下，我们可以采取逐步调优的方法。第一次实验可以先调节学习率和 dropout 率，找到一个相对不错的初始配置。然后在后续的实验中，可以再调节其他的超参数，进一步提高模型性能。

避免过度调优：如果调节的超参数过多，可能会导致调优过程变得复杂且时间较长，尤其是在数据量大的情况下。我们可以根据模型的实际需求逐步增加调节的超参数范围，避免过早引入过多的复杂度。

如何在未来的实验中增加更多超参数的调优？
在 run_tuning 函数中，可以加入更多的超参数，例如批次大小、卷积层的滤波器数量等。可以使用类似 optuna 这样的工具来动态地调整更多的超参数。
可以在调优过程中逐步增加超参数，例如在第一次实验中只调节 learning_rate 和 dropout_rate，在第二次实验中再加入 batch_size、卷积层的滤波器数量等。
